@startuml combined_prototype_evaluation_system
' ===== DATA MODULE =====
package "data" {
    abstract class BaseDataset {
        #outlier_fraction: float
        #noise_seed: int
        #use_gev: bool
        #data_splitter: DataSplitter
        +x_train: ndarray
        +y_train: ndarray
        +x_val: ndarray
        +y_val: ndarray
        +x_test: ndarray
        +y_test: ndarray
        +n_classes: int
        +__init__(**kwargs): void
        -{abstract} _load_data(): void
        -_load_and_preprocess(): void
    }

    class FordA_Dataset {
        -_load_data(): void
        +{static} _readucr(filename: str): tuple
    }

    class Wafer_Dataset {
        -_load_data(): void
        +{static} read_ariff(path: str): ndarray
    }

    class FordB_Dataset {
        -_load_data(): void
        +{static} _read_arff(path: str): tuple
    }

    class ECG200 {
        -_load_data(): void
        +{static} get_data(filepath: str): tuple
    }

    class BlinkDataset {
        +label_encoder: LabelEncoder
        -_load_data(): void
    }

    class SAWSINE_SEPERATE {
        -_load_data(): void
        +{static} get_data(): tuple
    }

    class StarLightCurve {
        -_load_data(): void
        +{static} _read_arff(path: str): tuple
    }

    class DatasetFactory {
        -{static} _datasets: Dict[str, Type]
        +{static} create_dataset(name: str, **kwargs): BaseDataset
        +{static} get_available_datasets(): list
    }

    class DataSplitter {
        +train_validation_split(x: ndarray, y: ndarray, test_size: float, random_state: int, stratify: bool): tuple
    }
}

' ===== DISTANCE MODULE =====
package "distance" {
    abstract class DistanceCalculator {
        +{abstract} compute_distance(X: ndarray, Y: ndarray): ndarray
    }

    class EuclideanDistance {
        +compute_distance(X: ndarray, Y: ndarray): ndarray
    }

    class DTWDistance {
        -decoder: Any
        +__init__(decoder: Any): void
        +compute_distance(X: ndarray, Y: ndarray): ndarray
    }

    class DistanceCalculatorFactory {
        -{static} _SUPPORTED_METRICS: Set[str]
        +{classmethod} create(metric: str, decoder: Optional): DistanceCalculator
    }
}

' ===== DTW UTILS MODULE =====
package "dtw_utils" {
    class DTWUtils {
        +{static} fast_dtw_core(x: ndarray, y: ndarray, window_size: int): float
        +{static} dtw_cdist_with_progress(X: ndarray, Y: ndarray, window_fraction: float, n_jobs: int): ndarray
    }
}

' ===== METRICS MODULE =====
package "metrics" {
    abstract class BaseEvaluationMetric {
        #distance_metric: str
        #distance_calculator: DistanceCalculator
        +__init__(distance_metric: str, decoder: Optional): void
        +{abstract} compute(**kwargs): float
        +_validate_inputs(**kwargs): void
    }

    class ConsistencyMetric {
        +compute(): Tuple[float, float]
        -_validate_consistency_inputs(): void
        -_compute_model_distances(): List[float]
        -_compute_consistency_error(): float
    }

    class ContinuityMetric {
        +DEFAULT_NOISE_SCALE: float
        +compute(): float
        -_validate_continuity_inputs(): void
        -_get_closest_prototypes(): ndarray
        -_add_noise(): ndarray
    }

    class ContrastivityMetric {
        +compute(prototypes: ndarray): float
    }

    class CorrectnessMetric {
        +compute(): Dict[str, float]
        -_validate_correctness_inputs(): void
        -_get_original_predictions(): ndarray
        -_get_prototype_predictions(): ndarray
        -_compute_classification_metrics(): Dict[str, float]
    }

    class CompactnessMetric {
        +COMPACTNESS_FACTOR: float
        +compute(prototypes: ndarray): float
    }

    class ConfidenceMetric {
        +compute(): float
    }

    class CohesionOfLatentSpaceMetric {
        +compute(): float
        -_can_compute_silhouette(): bool
        -_compute_silhouette_score(): float
        -_normalize_silhouette_score(): float
    }

    class InputCompletenessMetric {
        +compute(): float
        -_identify_matched_centroids(): List[bool]
        -_is_valid_centroid_match(): bool
    }

    class CovariateComplexityMetric {
        +compute(): float
        -_compute_prototype_silhouette_scores(): List[float]
        -_compute_centroids(): ndarray
        -_assign_prototypes_to_centroids(): ndarray
        -_compute_sample_silhouette_score(): float
    }
}

' ===== SCORE MODULE =====
package "score" {
    enum AggregationMethod {
        ARITHMETIC
        GEOMETRIC
        HARMONIC
    }

    class ScoreCalculator {
        +{static} compute_final_score(): float
        +{static} compute_score_breakdown(): Dict
    }

    class ScoreValidator {
        +{static} validate_results(): void
        +{static} validate_weights(): void
        +{static} validate_method(): AggregationMethod
    }

    class WeightNormalizer {
        +{static} filter_and_normalize(): tuple
    }

    class AggregationCalculator {
        +{static} calculate(): float
        -{static} _arithmetic_mean(): float
        -{static} _geometric_mean(): float
        -{static} _harmonic_mean(): float
    }
}
package "cluster" {
    class ClusteringConfig {
        +default_random_state: int
        +default_n_init: int
        +min_clusters: int
        +max_clusters: int
        +min_silhouette_samples: int
        +cluster_range: Tuple[int, int]
        +default_n_components: int
        +__init__(cfg: Any): void
    }

    class ValidationError <<Exception>> {
        +__init__(message: str): void
    }

    class ClusteringValidator {
        +{static} validate_data(data: ndarray, min_samples: int): void
        +{static} validate_cluster_params(n_clusters: int, data_size: int): void
        +{static} validate_cluster_range(cluster_range: Tuple[int, int], data_size: int, config: ClusteringConfig): Tuple[int, int]
    }

    class DimensionalityReducer {
        -{static} _REDUCERS: Dict[str, Callable]
        -method: str
        -config: ClusteringConfig
        +__init__(method: str, config: Optional[ClusteringConfig]): void
        +{classmethod} register_reducer(name: str, reducer_func: Callable): void
        +reduce_data(data: ndarray, n_components: Optional[int]): ndarray
        -_validate_method(): void
        -_get_default_reducer(): Callable
        -_reduce_with_pca(data: ndarray, n_components: int): ndarray
        -_reduce_with_tsne(data: ndarray, n_components: int): ndarray
        -_reduce_with_umap(data: ndarray, n_components: int): ndarray
    }

    class DataPreprocessor {
        -scale_data: bool
        -reduce_dimensions: bool
        -config: ClusteringConfig
        -reducer: Optional[DimensionalityReducer]
        +__init__(scale_data: bool, reduce_dimensions: bool, reduction_method: str, config: Optional[ClusteringConfig]): void
        +preprocess(data: ndarray, n_components: Optional[int]): ndarray
    }

    class ClusterOptimizer {
        -distance_metric: str
        -decoder: Any
        -config: ClusteringConfig
        -distance_calculator: DistanceCalculator
        -preprocessor: DataPreprocessor  // ADD THIS LINE
        +__init__(distance_metric: str, decoder: Any, config: Optional[ClusteringConfig]): void
        +find_optimal_clusters(data: ndarray, cluster_range: Optional[Tuple[int, int]]): Tuple[Optional[ndarray], float]
        -_optimize_euclidean_clustering(data: ndarray, cluster_range: Tuple[int, int]): Tuple[Optional[ndarray], float]
        -_optimize_dtw_clustering(data: ndarray, cluster_range: Tuple[int, int]): Tuple[Optional[ndarray], float]
        -_perform_kmeans_clustering(data: ndarray, n_clusters: int): ndarray
        -_perform_agglomerative_clustering(distance_matrix: ndarray, n_clusters: int): ndarray
        -_is_valid_clustering(labels: ndarray): bool
    }

    class ClusterMerger {
        +{static} merge_embeddings_and_labels(cluster_results: Dict[str, Dict[str, ndarray]]): Tuple[ndarray, ndarray]
}

    abstract class ClusteringStrategy {
        #distance_metric: str
        #decoder: Any
        #config: ClusteringConfig
        #optimizer: ClusterOptimizer
        #preprocessor: DataPreprocessor  // ADD THIS LINE
        +__init__(distance_metric: str, decoder: Any, clustering_config: Optional[ClusteringConfig]): void
        +{abstract} compute_multilevel_clustering(embeddings: ndarray, n_main_clusters: Optional[int], labels: Optional[ndarray], cluster_range: Optional[Tuple[int, int]]): Dict[str, Dict[str, ndarray]]
        #_get_main_cluster_labels(embeddings: ndarray, n_main_clusters: Optional[int], labels: Optional[ndarray]): ndarray
        #_perform_main_clustering(embeddings: ndarray, n_clusters: int): ndarray
        #_optimal_subclustering(class_embeddings: ndarray, cluster_range: Optional[Tuple[int, int]]): ndarray
    }

    class BasicClustering {
        -use_optimal_clusters: bool
        +__init__(distance_metric: str, decoder: Any, use_optimal_clusters: bool, clustering_config: Optional[ClusteringConfig]): void
        +compute_multilevel_clustering(embeddings: ndarray, n_main_clusters: Optional[int], labels: Optional[ndarray], cluster_range: Optional[Tuple[int, int]]): Dict[str, Dict[str, ndarray]]
    }

    class HierarchicalClustering {
        +__init__(distance_metric: str, decoder: Any, clustering_config: Optional[ClusteringConfig]): void
        +compute_multilevel_clustering(embeddings: ndarray, n_main_clusters: int, labels: Optional[ndarray]): Dict[str, Dict[str, ndarray]]
    }

    class ClusterMetricsCalculator {
        -config: ClusteringConfig
        +__init__(config: Optional[ClusteringConfig]): void
        +{static} compute_centroids(embeddings: ndarray, cluster_labels: ndarray): ndarray
        +compute_cluster_metrics_and_centroids(embeddings: ndarray, cluster_labels: ndarray): Dict[str, Any]
    }

    class PrototypeMapper {
        -distance_calculator: DistanceCalculator
        -config: ClusteringConfig
        +__init__(distance_metric: str, decoder: Any, config: Optional[ClusteringConfig]): void
        +compute_mapping(prototypes: ndarray, embeddings: ndarray, cluster_labels: ndarray): Tuple[Dict[int, Dict[str, Any]], Dict[int, Dict[str, Any]]]
        -_compute_centroid_info(embeddings: ndarray, cluster_labels: ndarray): Dict[int, Dict[str, Any]]
        -_extract_centroids(centroid_info: Dict[int, Dict[str, Any]]): ndarray
        -_map_prototypes_to_centroids(prototypes: ndarray, centroids: ndarray, centroid_info: Dict[int, Dict[str, Any]]): Dict[int, Dict[str, Any]]
    }

    class ClusteringFactory {
        +{static} create_clustering_strategy(distance_metric: str, decoder: Any, use_optimal_clusters: bool, clustering_config: Optional[ClusteringConfig]): ClusteringStrategy
    }
}

' ===== EVALUATORS MODULE =====
package "evaluators" {
    class EvaluationConfig {
        +distance_metric: str
        +criteria: dict
        +metric_weights: dict
        +visualize: bool
        +__init__(cfg: Any): void
        -_normalize_weights(weights_dict: dict): dict
    }

    abstract class BaseEvaluator {
        #config: EvaluationConfig
        #logger: Logger
        #evaluator: PrototypeEvaluator
        #prototype_extractor: PrototypeExtractor
        #config_manager: ConfigurationManager
        #pickle_handler: PickleHandler
        #visualizer: ClusteringVisualizer
        +__init__(config: EvaluationConfig): void
        +evaluate(weight_path: str, directory: str, label_legends: dict, dataset: BaseDataset, **kwargs): dict
        +{abstract} _load_model_components(weight_path: str, directory: str, dataset: BaseDataset, **kwargs): dict
        +{abstract} _extract_prototypes(directory: str, **kwargs): ndarray
        -_prepare_data(dataset: BaseDataset): SimpleNamespace
        -_run_evaluation(components: dict, prototypes: ndarray, data: SimpleNamespace, directory: str, label_legends: dict): dict
        -_visualize_results(results: dict, prototypes: ndarray, label_legends: dict, directory: str): void
    }

    class MAPEvaluator {
        -_load_model_components(weight_path: str, directory: str, dataset: BaseDataset, **kwargs): dict
        -_extract_prototypes(directory: str, **kwargs): ndarray
    }

    class MSPEvaluator {
        -_msp_prototypes: ndarray
        -_load_model_components(weight_path: str, directory: str, dataset: BaseDataset, **kwargs): dict
        -_extract_prototypes(directory: str, **kwargs): ndarray
    }

    class EBEEvaluator {
        -_load_model_components(weight_path: str, directory: str, dataset: BaseDataset, **kwargs): dict
        -_extract_prototypes(directory: str, **kwargs): ndarray
    }

    class EvaluatorFactory {
        -{static} _evaluators: Dict[str, Type]
        +{static} create_evaluator(method: str, config: EvaluationConfig): BaseEvaluator
        +{static} get_supported_methods(): list
    }

    class MethodEvaluator {
        +config: DictConfig
        +eval_config: EvaluationConfig
        +logger: Logger
        +__init__(config: DictConfig): void
        +validate_config(): void
        +run_evaluation(): Dict[str, Any]
        -_get_evaluation_params(dataset: BaseDataset): Dict[str, Any]
        -_get_ebe_params(): Dict[str, Any]
        -_get_msp_params(): Dict[str, Any]
        -_get_map_params(): Dict[str, Any]
    }

    class PrototypeEvaluator {
        -distance_metric: str
        -metric_weights: Dict[str, float] 
        -clustering_config: ClusteringConfig
        +__init__(distance_metric: str, metric_weights: Optional[Dict[str, float]], clustering_config: Optional[Dict[str, Any]]): void
        +evaluate(encoder: EncoderWrapper, decoder: DecoderWrapper, classifier: ClassifierWrapper, data: SimpleNamespace, prototypes: ndarray, save_path: str, criteria: Optional[Dict[str, bool]], **kwargs): Dict[str, Any]
        -_save_results(results: Dict[str, Any], save_path: str): void
        -_default_weights(): Dict[str, float]
    }
}

' ===== VISUALIZATION MODULE =====
package "visualization" {
    class PlotConfig {
        +__init__(cfg: Any): void
        +figure_size: Tuple[int, int]
        +xlabel_fontsize: int
        +ylabel_fontsize: int
        +tick_labelsize: int
        +legend_fontsize: int
        +legend_position: str
        +legend_bbox_to_anchor: Tuple[float, float]
        +legend_title_fontsize: int
        +marker_size_embeddings: int
        +marker_size_centroids: int
        +marker_size_prototypes: int
        +line_alpha: float
        +edge_linewidth: float
        +dpi: int
    }

    class VisualizationData <<dataclass>> {
        +embeddings_2d: ndarray
        +cluster_labels: ndarray
        +centroids_2d: ndarray
        +prototypes_2d: ndarray
        +class_label: str
    }

    abstract class DimensionReducer {
        +{abstract} fit_transform(data: ndarray): ndarray
        +{abstract} transform(data: ndarray): ndarray
    }

    class PCAReducer {
        -n_components: int
        -_fitted_reducer: PCA
        +fit_transform(data: ndarray): ndarray
        +transform(data: ndarray): ndarray
    }

    class TSNEReducer {
        -n_components: int
        -random_state: int
        -_fitted_reducer: TSNE
        +fit_transform(data: ndarray): ndarray
        +transform(data: ndarray): ndarray
    }

    class UMAPReducer {
        -n_components: int
        -random_state: int
        -_fitted_reducer: UMAP
        +fit_transform(data: ndarray): ndarray
        +transform(data: ndarray): ndarray
    }

    class ReducerFactory {
        -{static} _reducers: Dict[str, Type]
        +{static} create_reducer(method: str, **kwargs): DimensionReducer
    }

    class PlotRenderer {
        +config: PlotConfig
        +marker_shapes: list
        +colors: array
        +class_colors: Dict
        +cluster_markers: Dict
        +class_label_to_idx: Dict
        +get_class_color(class_label: str, idx: int): Tuple
        +get_cluster_marker(class_label: str, cluster: int): str
        +plot_embeddings(ax: Any, vis_data: VisualizationData, class_idx: int, label_legend: dict): void
        +plot_centroids(ax: Any, vis_data: VisualizationData, class_idx: int, label_legend: dict): void
        +plot_prototypes(ax: Any, prototypes_2d: ndarray, mapping: dict, centroid_idx_to_class_label: dict): void
        +plot_connections(ax: Any, prototypes_2d: ndarray, all_centroids_2d: ndarray, mapping: dict): void
        +add_legend(ax: Any, label_legend: dict): void
    }

    class ClusteringVisualizer {
        +config: PlotConfig
        +plot_renderer: PlotRenderer
        +visualize_clustering(output_clustering: dict, all_labels_metrics_centroids: dict, prototypes: ndarray, mapping: dict, label_legend: dict, method: str, save_path: str): void
        -_handle_none_centroids(all_labels_metrics_centroids: dict): Dict
        -_create_dummy_mapping(n_prototypes: int): Dict
        -_process_and_plot_classes(output_clustering: dict, all_labels_metrics_centroids: dict, reducer: DimensionReducer): Tuple
        -_prepare_class_data(embeddings: ndarray, cluster_labels: ndarray, centroids: ndarray, class_label: str, reducer: DimensionReducer): VisualizationData
        -_process_prototypes(prototypes: ndarray, reducer: DimensionReducer): ndarray
        -_configure_plot(ax: Any): void
        -_save_plot(save_path: str): void
    }
}

' ===== MAIN MODULE =====
package "main" {
    class MainEvaluator {
        +{static} main(cfg: DictConfig): void
        -{static} _log_configuration(cfg: DictConfig, evaluator: MethodEvaluator): void
        -{static} _log_results(results: dict): void
    }
}

' ===== HELPER MODULE =====
package "helper" {
    abstract class FileHandler {
        +{abstract} read(filepath: str): Any
        +{abstract} write(data: Any, filepath: str): void
    }

    class HDF5Handler {
        +read(filepath: str): Dict[str, Any]
        +write(data: Any, filepath: str): void
        +inspect_file(filepath: str): void
    }

    class PickleHandler {
        +read(filepath: str): Any
        +write(data: Any, filepath: str): void
    }

    class CSVHandler {
        +read(filepath: str): pd.DataFrame
        +write(data: pd.DataFrame, filepath: str): void
    }

    class JSONHandler {
        +read(filepath: str): Dict[str, Any]
        +write(data: Dict[str, Any], filepath: str): void
    }

    class FileHandlerFactory {
        -{static} _handlers: Dict[str, Type]
        +{static} create_handler(filepath: str): FileHandler
    }

    class PrototypeExtractor {
        -_factory: FileHandlerFactory
        +__init__(file_handler_factory: FileHandlerFactory): void
        +extract_from_csv_map(filepath: str): ndarray
        -_parse_latent_centers_string(string_data: str): ndarray
    }

    class ConfigurationManager {
        -_json_handler: JSONHandler
        +__init__(json_handler: JSONHandler): void
        +load_from_json(json_path: str): argparse.Namespace
        -_create_parser_from_config(config: Dict[str, Any]): argparse.ArgumentParser
    }
}

' ===== WRAPPER MODULE =====
package "wrapper" {
    abstract class ModelWrapper {
        #_model: Callable
        +__init__(model: Callable): void
        +{abstract} call(inputs: tf.Tensor): tf.Tensor
        +__call__(inputs: tf.Tensor): tf.Tensor
        +model: Callable
    }

    class EncoderWrapper {
        +call(inputs: tf.Tensor): tf.Tensor
    }

    class DecoderWrapper {
        +call(inputs: tf.Tensor): tf.Tensor
    }

    class ClassifierWrapper {
        +call(inputs: tf.Tensor): tf.Tensor
    }

    class CompositeClassifier {
        -_encoder: Callable
        -_predictor: Callable
        +__init__(encoder: Callable, predictor: Callable, name: str, **kwargs): void
        +call(inputs: tf.Tensor, training: Optional[bool]): tf.Tensor
        +encoder: Callable
        +predictor: Callable
    }

    class WrapperFactory {
        +{static} create_encoder_wrapper(model: Callable, model_type: str): EncoderWrapper
        +{static} create_decoder_wrapper(model: Callable): DecoderWrapper
        +{static} create_classifier_wrapper(model: Callable, model_type: str, **kwargs): ClassifierWrapper
    }
}

' ===== RELATIONSHIPS =====

' Data inheritance hierarchy
BaseDataset <|-- FordA_Dataset
BaseDataset <|-- Wafer_Dataset  
BaseDataset <|-- FordB_Dataset
BaseDataset <|-- ECG200
BaseDataset <|-- BlinkDataset
BaseDataset <|-- SAWSINE_SEPERATE
BaseDataset <|-- StarLightCurve

' Distance interface implementations
DistanceCalculator <|-- EuclideanDistance
DistanceCalculator <|-- DTWDistance
DTWDistance ..> DTWUtils : uses

' Metric inheritance hierarchy
BaseEvaluationMetric <|-- ConsistencyMetric
BaseEvaluationMetric <|-- ContinuityMetric
BaseEvaluationMetric <|-- ContrastivityMetric
BaseEvaluationMetric <|-- CorrectnessMetric
BaseEvaluationMetric <|-- CompactnessMetric
BaseEvaluationMetric <|-- ConfidenceMetric
BaseEvaluationMetric <|-- CohesionOfLatentSpaceMetric
BaseEvaluationMetric <|-- InputCompletenessMetric
BaseEvaluationMetric <|-- CovariateComplexityMetric

' Clustering hierarchy
ClusteringStrategy <|-- BasicClustering
ClusteringStrategy <|-- HierarchicalClustering

' Evaluator inheritance hierarchy
BaseEvaluator <|-- MAPEvaluator
BaseEvaluator <|-- MSPEvaluator
BaseEvaluator <|-- EBEEvaluator

' Visualization inheritance hierarchy
DimensionReducer <|-- PCAReducer
DimensionReducer <|-- TSNEReducer
DimensionReducer <|-- UMAPReducer

' Helper inheritance hierarchy
FileHandler <|-- HDF5Handler
FileHandler <|-- PickleHandler
FileHandler <|-- CSVHandler
FileHandler <|-- JSONHandler

' Wrapper inheritance hierarchy
ModelWrapper <|-- EncoderWrapper
ModelWrapper <|-- DecoderWrapper
ModelWrapper <|-- ClassifierWrapper

' Factory pattern relationships
DatasetFactory ..> BaseDataset : creates
DistanceCalculatorFactory ..> DistanceCalculator : creates
EvaluatorFactory ..> BaseEvaluator : creates
ReducerFactory ..> DimensionReducer : creates
FileHandlerFactory ..> FileHandler : creates
WrapperFactory ..> ModelWrapper : creates
ClusteringFactory ..> ClusteringStrategy : creates

' Core evaluation dependencies
BaseDataset ..> DataSplitter : uses
BaseEvaluator ..> EvaluationConfig : uses
BaseEvaluator ..> PrototypeEvaluator : uses
BaseEvaluator ..> ClusteringVisualizer : uses


' Score calculation dependencies
ScoreCalculator ..> ScoreValidator : uses
ScoreCalculator ..> WeightNormalizer : uses
ScoreCalculator ..> AggregationCalculator : uses
AggregationCalculator ..> AggregationMethod : uses

' Clustering dependencies
ClusteringStrategy ..> ClusterOptimizer : uses
ClusteringStrategy ..> ClusterMetricsCalculator : uses
ClusterOptimizer ..> ClusteringValidator : uses
DataPreprocessor --> DimensionalityReducer : contains

' Method evaluator coordination
MethodEvaluator ..> EvaluationConfig : uses
MethodEvaluator ..> EvaluatorFactory : uses
MethodEvaluator ..> DatasetFactory : uses

' Visualization dependencies
ClusteringVisualizer ..> PlotConfig : uses
ClusteringVisualizer ..> PlotRenderer : uses
ClusteringVisualizer ..> ReducerFactory : uses
ClusteringVisualizer ..> VisualizationData : creates
PlotRenderer ..> PlotConfig : uses

' Configuration usage
BasicClustering ..> ClusteringConfig : uses
HierarchicalClustering ..> ClusteringConfig : uses
ClusterOptimizer ..> ClusteringConfig : uses
ClusterOptimizer --> DataPreprocessor : contains
ClusteringStrategy --> DataPreprocessor : contains

' Main orchestrator
MainEvaluator ..> MethodEvaluator : uses

' Prototype evaluator orchestration
PrototypeEvaluator --> ClusteringFactory : uses
PrototypeEvaluator --> PrototypeMapper : uses
PrototypeEvaluator --> ClusterMerger : uses
PrototypeEvaluator --> BaseEvaluationMetric : creates
PrototypeEvaluator --> ScoreCalculator : uses

' Helper dependencies
PrototypeExtractor ..> FileHandlerFactory : uses
ConfigurationManager ..> JSONHandler : uses

' Cross-module relationships
BaseEvaluator ..> PrototypeExtractor : uses
BaseEvaluator ..> ConfigurationManager : uses

' Core evaluator uses wrapper interface
PrototypeEvaluator ..> ModelWrapper : uses

' Method evaluators use wrapper interface
MAPEvaluator ..> ModelWrapper : uses
MSPEvaluator ..> ModelWrapper : uses  
EBEEvaluator ..> ModelWrapper : uses

@enduml